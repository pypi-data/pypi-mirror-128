# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class DatabricksConfiguration(Model):
    """DatabricksConfiguration.

    :param workers:
    :type workers: int
    :param minimum_worker_count:
    :type minimum_worker_count: int
    :param max_mum_worker_count:
    :type max_mum_worker_count: int
    :param spark_version:
    :type spark_version: str
    :param node_type_id:
    :type node_type_id: str
    :param spark_conf:
    :type spark_conf: dict[str, str]
    :param spark_env_vars:
    :type spark_env_vars: dict[str, str]
    :param cluster_log_conf_dbfs_path:
    :type cluster_log_conf_dbfs_path: str
    :param dbfs_init_scripts:
    :type dbfs_init_scripts: list[~designer.models.InitScriptInfoDto]
    :param instance_pool_id:
    :type instance_pool_id: str
    :param timeout_seconds:
    :type timeout_seconds: int
    :param notebook_task:
    :type notebook_task: ~designer.models.NoteBookTaskDto
    :param spark_python_task:
    :type spark_python_task: ~designer.models.SparkPythonTaskDto
    :param spark_jar_task:
    :type spark_jar_task: ~designer.models.SparkJarTaskDto
    :param spark_submit_task:
    :type spark_submit_task: ~designer.models.SparkSubmitTaskDto
    :param libraries:
    :type libraries: list[object]
    :param linked_adb_workspace_metadata:
    :type linked_adb_workspace_metadata:
     ~designer.models.LinkedADBWorkspaceMetadata
    :param databrick_resource_id:
    :type databrick_resource_id: str
    """

    _attribute_map = {
        'workers': {'key': 'workers', 'type': 'int'},
        'minimum_worker_count': {'key': 'minimumWorkerCount', 'type': 'int'},
        'max_mum_worker_count': {'key': 'maxMumWorkerCount', 'type': 'int'},
        'spark_version': {'key': 'sparkVersion', 'type': 'str'},
        'node_type_id': {'key': 'nodeTypeId', 'type': 'str'},
        'spark_conf': {'key': 'sparkConf', 'type': '{str}'},
        'spark_env_vars': {'key': 'sparkEnvVars', 'type': '{str}'},
        'cluster_log_conf_dbfs_path': {'key': 'clusterLogConfDbfsPath', 'type': 'str'},
        'dbfs_init_scripts': {'key': 'dbfsInitScripts', 'type': '[InitScriptInfoDto]'},
        'instance_pool_id': {'key': 'instancePoolId', 'type': 'str'},
        'timeout_seconds': {'key': 'timeoutSeconds', 'type': 'int'},
        'notebook_task': {'key': 'notebookTask', 'type': 'NoteBookTaskDto'},
        'spark_python_task': {'key': 'sparkPythonTask', 'type': 'SparkPythonTaskDto'},
        'spark_jar_task': {'key': 'sparkJarTask', 'type': 'SparkJarTaskDto'},
        'spark_submit_task': {'key': 'sparkSubmitTask', 'type': 'SparkSubmitTaskDto'},
        'libraries': {'key': 'libraries', 'type': '[object]'},
        'linked_adb_workspace_metadata': {'key': 'linkedADBWorkspaceMetadata', 'type': 'LinkedADBWorkspaceMetadata'},
        'databrick_resource_id': {'key': 'databrickResourceId', 'type': 'str'},
    }

    def __init__(self, *, workers: int=None, minimum_worker_count: int=None, max_mum_worker_count: int=None, spark_version: str=None, node_type_id: str=None, spark_conf=None, spark_env_vars=None, cluster_log_conf_dbfs_path: str=None, dbfs_init_scripts=None, instance_pool_id: str=None, timeout_seconds: int=None, notebook_task=None, spark_python_task=None, spark_jar_task=None, spark_submit_task=None, libraries=None, linked_adb_workspace_metadata=None, databrick_resource_id: str=None, **kwargs) -> None:
        super(DatabricksConfiguration, self).__init__(**kwargs)
        self.workers = workers
        self.minimum_worker_count = minimum_worker_count
        self.max_mum_worker_count = max_mum_worker_count
        self.spark_version = spark_version
        self.node_type_id = node_type_id
        self.spark_conf = spark_conf
        self.spark_env_vars = spark_env_vars
        self.cluster_log_conf_dbfs_path = cluster_log_conf_dbfs_path
        self.dbfs_init_scripts = dbfs_init_scripts
        self.instance_pool_id = instance_pool_id
        self.timeout_seconds = timeout_seconds
        self.notebook_task = notebook_task
        self.spark_python_task = spark_python_task
        self.spark_jar_task = spark_jar_task
        self.spark_submit_task = spark_submit_task
        self.libraries = libraries
        self.linked_adb_workspace_metadata = linked_adb_workspace_metadata
        self.databrick_resource_id = databrick_resource_id
