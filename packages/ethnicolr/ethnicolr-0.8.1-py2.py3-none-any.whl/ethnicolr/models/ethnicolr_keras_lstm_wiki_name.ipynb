{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asian,GreaterEastAsian,EastAsian</th>\n",
       "      <td>5497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian,GreaterEastAsian,Japanese</th>\n",
       "      <td>7333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian,IndianSubContinent</th>\n",
       "      <td>7861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterAfrican,Africans</th>\n",
       "      <td>3672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterAfrican,Muslim</th>\n",
       "      <td>6242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,British</th>\n",
       "      <td>41445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,EastEuropean</th>\n",
       "      <td>8329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,Jewish</th>\n",
       "      <td>10239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,French</th>\n",
       "      <td>12293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Germanic</th>\n",
       "      <td>3869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Hispanic</th>\n",
       "      <td>10412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Italian</th>\n",
       "      <td>11867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GreaterEuropean,WestEuropean,Nordic</th>\n",
       "      <td>4813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name_first\n",
       "race                                             \n",
       "Asian,GreaterEastAsian,EastAsian             5497\n",
       "Asian,GreaterEastAsian,Japanese              7333\n",
       "Asian,IndianSubContinent                     7861\n",
       "GreaterAfrican,Africans                      3672\n",
       "GreaterAfrican,Muslim                        6242\n",
       "GreaterEuropean,British                     41445\n",
       "GreaterEuropean,EastEuropean                 8329\n",
       "GreaterEuropean,Jewish                      10239\n",
       "GreaterEuropean,WestEuropean,French         12293\n",
       "GreaterEuropean,WestEuropean,Germanic        3869\n",
       "GreaterEuropean,WestEuropean,Hispanic       10412\n",
       "GreaterEuropean,WestEuropean,Italian        11867\n",
       "GreaterEuropean,WestEuropean,Nordic          4813"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "EPOCHS = 20\n",
    "\n",
    "# Wikilabels\n",
    "df = pd.read_csv('../data/wiki/wiki_name_race.csv')\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "sdf = df\n",
    "\n",
    "# Additional features\n",
    "sdf['name_first'] = sdf.name_first.str.title()\n",
    "sdf['name_last'] = sdf.name_last.str.title()\n",
    "\n",
    "rdf = sdf.groupby('race').agg({'name_first': 'count'})\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.to_csv('./wiki/lstm/wiki_name_race.csv', columns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 2210\n",
      "Max feature len = 74, Avg. feature len = 12\n"
     ]
    }
   ],
   "source": [
    "# concat last name and first name\n",
    "sdf['name_last_name_first'] = sdf['name_last'] + ' ' + sdf['name_first']\n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(sdf.name_last_name_first)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = [w[1] for w in words]\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(sdf.name_last_name_first.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM model\n",
    "\n",
    "ref: http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107097 train sequences\n",
      "26775 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (107097, 25)\n",
      "X_test shape: (26775, 25)\n",
      "13 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (107097, 13)\n",
      "y_test shape: (26775, 13)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 25 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 32)            70720     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 154,829\n",
      "Trainable params: 154,829\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 32, input_length=feature_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/20\n",
      "3013/3013 [==============================] - 141s 38ms/step - loss: 1.6700 - accuracy: 0.4921 - val_loss: 1.1365 - val_accuracy: 0.6692\n",
      "Epoch 2/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 1.1289 - accuracy: 0.6689 - val_loss: 1.0618 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 1.0534 - accuracy: 0.6913 - val_loss: 1.0353 - val_accuracy: 0.6999\n",
      "Epoch 4/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 1.0160 - accuracy: 0.7051 - val_loss: 1.0087 - val_accuracy: 0.7110\n",
      "Epoch 5/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 0.9787 - accuracy: 0.7167 - val_loss: 0.9938 - val_accuracy: 0.7157\n",
      "Epoch 6/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 0.9484 - accuracy: 0.7250 - val_loss: 0.9810 - val_accuracy: 0.7203\n",
      "Epoch 7/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 0.9272 - accuracy: 0.7312 - val_loss: 0.9728 - val_accuracy: 0.7218\n",
      "Epoch 8/20\n",
      "3013/3013 [==============================] - 113s 37ms/step - loss: 0.9065 - accuracy: 0.7376 - val_loss: 0.9764 - val_accuracy: 0.7210\n",
      "Epoch 9/20\n",
      "3013/3013 [==============================] - 113s 37ms/step - loss: 0.8919 - accuracy: 0.7394 - val_loss: 0.9640 - val_accuracy: 0.7271\n",
      "Epoch 10/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8626 - accuracy: 0.7503 - val_loss: 0.9560 - val_accuracy: 0.7322\n",
      "Epoch 11/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8612 - accuracy: 0.7504 - val_loss: 0.9540 - val_accuracy: 0.7324\n",
      "Epoch 12/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8416 - accuracy: 0.7562 - val_loss: 0.9577 - val_accuracy: 0.7312\n",
      "Epoch 13/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8342 - accuracy: 0.7570 - val_loss: 0.9600 - val_accuracy: 0.7339\n",
      "Epoch 14/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8257 - accuracy: 0.7605 - val_loss: 0.9533 - val_accuracy: 0.7331\n",
      "Epoch 15/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8074 - accuracy: 0.7630 - val_loss: 0.9561 - val_accuracy: 0.7338\n",
      "Epoch 16/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.8078 - accuracy: 0.7641 - val_loss: 0.9524 - val_accuracy: 0.7341\n",
      "Epoch 17/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.7939 - accuracy: 0.7671 - val_loss: 0.9573 - val_accuracy: 0.7346\n",
      "Epoch 18/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.7880 - accuracy: 0.7684 - val_loss: 0.9556 - val_accuracy: 0.7323\n",
      "Epoch 19/20\n",
      "3013/3013 [==============================] - 114s 38ms/step - loss: 0.7929 - accuracy: 0.7671 - val_loss: 0.9587 - val_accuracy: 0.7350\n",
      "Epoch 20/20\n",
      "3013/3013 [==============================] - 113s 38ms/step - loss: 0.7772 - accuracy: 0.7712 - val_loss: 0.9666 - val_accuracy: 0.7386\n",
      "837/837 [==============================] - 5s 6ms/step - loss: 0.9647 - accuracy: 0.7382\n",
      "Test score: 0.9646637439727783\n",
      "Test accuracy: 0.7381512522697449\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837/837 - 6s\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "     Asian,GreaterEastAsian,EastAsian       0.87      0.80      0.83      1099\n",
      "      Asian,GreaterEastAsian,Japanese       0.89      0.90      0.89      1467\n",
      "             Asian,IndianSubContinent       0.82      0.75      0.78      1572\n",
      "              GreaterAfrican,Africans       0.59      0.43      0.50       734\n",
      "                GreaterAfrican,Muslim       0.66      0.72      0.69      1248\n",
      "              GreaterEuropean,British       0.76      0.89      0.82      8289\n",
      "         GreaterEuropean,EastEuropean       0.78      0.74      0.76      1666\n",
      "               GreaterEuropean,Jewish       0.50      0.46      0.48      2048\n",
      "  GreaterEuropean,WestEuropean,French       0.72      0.59      0.65      2459\n",
      "GreaterEuropean,WestEuropean,Germanic       0.52      0.43      0.47       774\n",
      "GreaterEuropean,WestEuropean,Hispanic       0.73      0.69      0.71      2082\n",
      " GreaterEuropean,WestEuropean,Italian       0.76      0.75      0.76      2374\n",
      "  GreaterEuropean,WestEuropean,Nordic       0.78      0.66      0.71       963\n",
      "\n",
      "                             accuracy                           0.74     26775\n",
      "                            macro avg       0.72      0.68      0.70     26775\n",
      "                         weighted avg       0.73      0.74      0.73     26775\n",
      "\n",
      "[[ 877   45    8    4   10  107    5   11   11    3    6    5    7]\n",
      " [  19 1319    5   11    4   46    8    5    2    2   33   11    2]\n",
      " [   9   12 1174   16  137  117   17   24   14   12   16   16    8]\n",
      " [   4   37   35  316   84  112   12   33   33    6   41   17    4]\n",
      " [   6    6   74   22  898   91   34   65   19    5   11   10    7]\n",
      " [  51   12   48   49   53 7349   44  276  157   51   66   87   46]\n",
      " [   6    7   18   11   26   94 1240  140   27   38   12   35   12]\n",
      " [   5    8   16   23   78  606  108  935   70   64   78   41   16]\n",
      " [   9    8   16   36   40  479   34  127 1462   30   99  109   10]\n",
      " [   5    1    5    3    5  161   24  108   39  334   26   22   41]\n",
      " [   8   18   20   17   16  184   17   67   76   19 1446  182   12]\n",
      " [   5   13   15   23    8  183   21   38   96   27  151 1777   17]\n",
      " [   3    2    3    8   10  151   23   39   16   48    8   15  637]]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test, verbose=2) # to predict probability\n",
    "y_pred = np.argmax(p, axis=-1)\n",
    "target_names = list(sdf.race.astype('category').cat.categories)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./wiki/lstm/wiki_name_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('./wiki/lstm/wiki_name_vocab.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
